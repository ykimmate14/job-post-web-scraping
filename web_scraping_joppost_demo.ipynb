{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program asks a user to enter a **job title** and **location** of interest and scrapes relavant data from [a job posting website](www.indeed.com). Using NLTK, it removes stopwords and count the splited words. Then it asks a user to enter specific set of skills(separated by space) and return the sorted list of **skills** with number of occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests # for accesing web page\n",
    "from bs4 import BeautifulSoup # for pulling data out of html\n",
    "import pandas as pd # for general working with data\n",
    "# from nltk import word_tokenize # text mining / analysis\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import re #regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function take jobtitle and location as arguments and return correct url for web scraping purpose\n",
    "def searchquery(jobtitle, location):\n",
    "    title = jobtitle.replace(' ', '+')\n",
    "    loc = location.replace(' ', '+')\n",
    "    url = 'http://www.indeed.com/jobs?q=%22'+ title +'%22&radius=50&limit=50&l='+loc\n",
    "    return url\n",
    "\n",
    "# A function to take job list's url as an input and return dataframe with job titles and url to job post\n",
    "def collect_job_list(url):\n",
    "    # create empty list\n",
    "    jobtitle, hreflink = [], []\n",
    "    \n",
    "    # get contents from the web\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    # find the page number\n",
    "    x = soup.findAll('div', {'id': 'searchCount'})[0].text.replace(',', '')\n",
    "    pageN = int(x[x.find('of ')+3:])\n",
    "    \n",
    "    # iterate over page number    \n",
    "    for i in range(0, pageN, 50):\n",
    "        joblisturl = url + '&start=' + str(i)\n",
    "        r = requests.get(joblisturl)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        \n",
    "        # iterate over each listed job post on search result to obtain job title and link\n",
    "        for data in soup.findAll('a', {'data-tn-element': 'jobTitle'}):\n",
    "            if 'clk?jk=' in data.get('href'):\n",
    "                hreflink.append(data.get('href'))\n",
    "                jobtitle.append(data.text)\n",
    "    df = pd.DataFrame({'title': jobtitle, 'link': hreflink})\n",
    "    return df\n",
    "\n",
    "\n",
    "# convert the href link data in dataframe to proper url\n",
    "def properurl(link):\n",
    "    joburl = 'http://www.indeed.com/viewjob?jk=' +\\\n",
    "            link[link.find('clk?jk=')+len('clk?jk='):link.find('&fccid')]\n",
    "    return joburl\n",
    "            \n",
    "\n",
    "# A function to take job posting's url as an input, mine text data from selected job post. \n",
    "# and return the text from the post.\n",
    "def collect_job_data(joblink_list):\n",
    "    jobdesc = []\n",
    "    #iterate over href link in data frame\n",
    "    for i in range(0, len(joblink_list)):\n",
    "        joburl = properurl(joblink_list[i])\n",
    "        \n",
    "        #extracting text data from selected job posting        \n",
    "        r = requests.get(joburl)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        desc = ''.join(soup.findAll('td', {'class': 'snip'})[0].text)\n",
    "        desc = re.sub('[^A-Za-z0-9&]+', ' ', desc)\n",
    "        jobdesc.append(desc[:desc.find('ago')].replace('\\n', ' ').lower())\n",
    "    return jobdesc\n",
    "\n",
    "# A function to take str as input, split the str and count the words\n",
    "def countword(text):\n",
    "    #removing stopwords from the data\n",
    "    stop = stopwords.words('english')   \n",
    "    \n",
    "    nostopword = ' '.join([word for word in text.split() if word not in stop])\n",
    "    #create word count list\n",
    "    count = Counter(nostopword.split())\n",
    "    return count\n",
    "\n",
    "# A function to take words as input and return the list of counts for the words of interest.\n",
    "def sortlist(words, countlist):\n",
    "    result= []  \n",
    "    for word in words.split():\n",
    "        result.append([x for x in countlist if word in x])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the \"job title\" of interest:  data analyst\n",
      "Please enter the \"location\" of interest:  california\n",
      "Number of open position: 513\n",
      "\n",
      "The 100 most frequent words are as below:\n",
      "\n",
      "[('data', 4430), ('experience', 1909), ('business', 1504), ('skills', 1046), ('work', 1034), ('team', 920), ('analysis', 897), ('ability', 866), ('analyst', 768), ('management', 658), ('support', 595), ('strong', 568), ('knowledge', 549), ('years', 540), ('reporting', 538), ('analytics', 527), ('information', 515), ('sql', 507), ('required', 504), ('requirements', 497), ('days', 480), ('tools', 479), ('working', 456), ('development', 446), ('reports', 429), ('product', 427), ('including', 422), ('quality', 403), ('related', 401), ('systems', 390), ('technical', 388), ('position', 377), ('develop', 373), ('environment', 369), ('customer', 358), ('degree', 358), ('job', 350), ('preferred', 339), ('new', 337), ('provide', 335), ('&', 334), ('company', 332), ('analytical', 331), ('using', 327), ('communication', 324), ('insights', 318), ('projects', 316), ('understanding', 300), ('marketing', 299), ('process', 298), ('responsibilities', 297), ('must', 295), ('excel', 289), ('solutions', 289), ('teams', 284), ('project', 281), ('health', 280), ('time', 273), ('design', 272), ('processes', 270), ('education', 267), ('role', 263), ('services', 262), ('key', 259), ('complex', 255), ('analyze', 251), ('statistical', 249), ('help', 248), ('manage', 242), ('30', 242), ('create', 239), ('5', 239), ('software', 238), ('science', 238), ('within', 237), ('3', 237), ('qualifications', 236), ('perform', 234), ('plus', 233), ('results', 233), ('based', 233), ('research', 231), ('performance', 231), ('etc', 230), ('client', 230), ('opportunity', 229), ('identify', 229), ('computer', 228), ('organization', 227), ('high', 227), ('system', 225), ('excellent', 225), ('database', 224), ('technology', 223), ('financial', 223), ('use', 223), ('across', 223), ('2', 220), ('responsible', 218), ('looking', 216)]\n"
     ]
    }
   ],
   "source": [
    "# prompt a user to input job title and location of interest\n",
    "jobtitle = input('Please enter the \"job title\" of interest:  ') \n",
    "location = input('Please enter the \"location\" of interest:  ')\n",
    "\n",
    "# using the 'collect_job_list' funciton, assign data frame to a variable 'df' \n",
    "df = collect_job_list(searchquery(jobtitle, location))\n",
    "\n",
    "# remove duplicated data\n",
    "df = df[df.duplicated('link') == False]\n",
    "\n",
    "# add column of text that contains job description to the dataframe\n",
    "df['text'] = collect_job_data(list(df.link))\n",
    "\n",
    "# Clean data with text whose length is less than 100\n",
    "for index, row in df.iterrows():\n",
    "    if(len(row.text) < 100):\n",
    "        row.text = ''\n",
    "df = df[df.text != '']\n",
    "\n",
    "# iterate over the dataframe to split the texts into words\n",
    "for index, row in df.iterrows():\n",
    "    row.text = countword(row.text)\n",
    "\n",
    "# count the occurence of words\n",
    "totalcount = Counter()\n",
    "for index, row in df.iterrows():\n",
    "    totalcount += row.text\n",
    "\n",
    "# reorder the list of count in descending order\n",
    "result = totalcount.most_common()\n",
    "\n",
    "# overview of the most frequent words\n",
    "print(\"Number of open position: \"+str(len(df)))\n",
    "print(\"\\nThe 100 most frequent words are as below:\\n\")\n",
    "print(result[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the words of interst separated by space\n",
      "sql r python hadoop excel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('sql', 507)],\n",
       " [('r', 104)],\n",
       " [('python', 110)],\n",
       " [('hadoop', 57)],\n",
       " [('excel', 289)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt a user to sort the list with words of interest\n",
    "intwords = input('Enter the words of interst separated by space\\n')\n",
    "sortlist(intwords, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
